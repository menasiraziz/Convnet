{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convnet2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCLBBmXDuyhx0Bmmr8zuhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menasiraziz/Convnet/blob/work/convnet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6SP1-Wn0LPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chEltUT71tFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "\n",
        "def strided_convolution3D(image, weight, stride):\n",
        "    m,ch,im_h, im_w = image.shape\n",
        "    f,t,f_h, f_w = weight.shape\n",
        "    out_shape = (m,ch,1 + (im_h - f_h) // stride, 1 + (im_w - f_w) // stride, f_h, f_w)\n",
        "    out_strides = (image.strides[0],image.strides[1],image.strides[2] * stride, image.strides[3] * stride, image.strides[2], image.strides[3])\n",
        "    windows = as_strided(image, shape=out_shape, strides=out_strides)\n",
        "    return np.einsum('mcopjk,ecjk->meop',windows,weight)\n",
        "\n",
        "def mx_pool(img,s): # function takes advatange of row order of numpy array\n",
        "  m,ch,n,n=img.shape # flatten all images into stride \n",
        "  c=img.reshape(-1,s) #change s to change horizontal stride\n",
        "  ind=np.argmax(c,axis=1)\n",
        "  ind1=np.ravel_multi_index([np.arange(ind.shape[0]),ind], (ind.shape[0],s))\n",
        "  d=img.flatten()[ind1]\n",
        "  g=d.reshape(-1,s,int(n/s)) #probably i need to change s in the middle to change vertical stride\n",
        "\n",
        "  h=g.argmax(axis=1)\n",
        "  i1=np.arange(g.shape[0]) # varies along images in data \n",
        "  i2=np.arange(g.shape[2]) # varies along col axis in indices\n",
        "  ss=i1[:,np.newaxis]*n+g.shape[2]*h+i2 # i2 is varying along col and i1 is varying along batch and row dim\n",
        "  return ind1[ss.flatten()]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "\n",
        "def strided_convolution3D1_g(image, weight, stride):\n",
        "    m,ch,im_h, im_w = image.shape\n",
        "    f,t,f_h, f_w = weight.shape\n",
        "    #print(m)\n",
        "    out_shape = (m,ch,1 + (im_h - f_h) // stride, 1 + (im_w - f_w) // stride, f_h, f_w)\n",
        "    out_strides = (image.strides[0],image.strides[1],image.strides[2] * stride, image.strides[3] * stride, image.strides[2], image.strides[3])\n",
        "    windows = as_strided(image, shape=out_shape, strides=out_strides)\n",
        "    #print(windows.shape)\n",
        "    #print(windows)\n",
        "    return np.einsum('meopjk,ecjk->mecop',windows,weight)\n",
        "    #return np.einsum('meopjk,ecjk->meop',windows,weight)\n",
        "    #return np.einsum('meopjk,ecjk->mecop',windows,weight)\n",
        "\n",
        "def strided_convolution3D_grad1(image, weight, stride):\n",
        "    m,ch,im_h, im_w = image.shape\n",
        "    m1,m2,f_h, f_w = weight.shape\n",
        "    \n",
        "    #print(m)\n",
        "    out_shape = (m,ch,1 + (im_h - f_h) // stride, 1 + (im_w - f_w) // stride, f_h, f_w)\n",
        "    out_strides = (image.strides[0],image.strides[1],image.strides[2] * stride, image.strides[3] * stride, image.strides[2], image.strides[3])\n",
        "    windows = as_strided(image, shape=out_shape, strides=out_strides)\n",
        "    #print(windows.shape)\n",
        "    #print(windows[0])\n",
        "    #print(windows*gg1)\n",
        "    return np.einsum('mcopjk,mejk->mecop',windows,weight)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWRjGVX0PeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "\n",
        "class FC_Layer:\n",
        "  def __init__(self,i_dim,out_dim):\n",
        "    self.W=np.random.randn(i_dim,out_dim)\n",
        "    self.dw=np.zeros(self.W.shape)\n",
        "  def feedforward(self,x):\n",
        "    self.out=np.dot(x,self.W)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    return self.W*loss_grad[:,np.newaxis]\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=7\n",
        "    self.in_ch=1\n",
        "    self.m=4\n",
        "    self.f1=2\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=1\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=1,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "    self.FC1=FC_Layer(cos*cos,1)\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    self.m1_indices=mx_pool(c1,self.mxs)\n",
        "    self.m1=c1.flatten()[self.m1_indices].reshape(-1,c1.shape[1],int(c1.shape[2]/self.mxs),int(c1.shape[3]/self.mxs))\n",
        "    c2=self.conv2.feedforward(self.m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def backpropagate(self,x,y,yhat):\n",
        "    loss_grad=2*(yhat-y)\n",
        "    print(loss_grad.shape)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    print(gradF1.shape)\n",
        "    gradC2=self.conv2.grad(self.m1,gradF1)\n",
        "    print(gradC2.shape)\n",
        "    grad_zeros=np.zeros(self.conv1.out.shape[0]*self.conv1.out.shape[1]*self.conv1.out.shape[2]*self.conv1.out.shape[3])\n",
        "    grad_zeros[self.m1_indices]=gradC2.flatten()\n",
        "    gg=grad_zeros.reshape(self.conv1.out.shape)\n",
        "    gradC1=self.conv1.grad(x,gg)\n",
        "    print(gradC1)\n",
        "\n",
        "  def loss(self,y,yhat):\n",
        "    return np.sum(np.square(y-yhat))\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.loss(y,yhat1)-self.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uQyJjngmRn_",
        "colab_type": "code",
        "outputId": "6de18f2f-b471-42c1-87b7-82428e361673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(100)\n",
        "nn=Network()\n",
        "img=nn.gen_images()*10\n",
        "y=np.random.randn(img.shape[0],1)\n",
        "yhat=nn.feedforward(img)\n",
        "nn.backpropagate(img,y,yhat)\n",
        "nn.num_grad(img,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 1)\n",
            "(4, 1, 2, 2)\n",
            "(4, 1, 3, 3)\n",
            "[[[[  0.           0.          -5.89396339   1.15429512   0.\n",
            "      0.           0.        ]\n",
            "   [  7.17309223  -1.40480435   3.88392097  -0.85031324  -5.05808803\n",
            "      0.99059427   0.        ]\n",
            "   [ 14.22787138  -2.67730476   0.         -17.38452561   6.73775601\n",
            "     -0.72972276   0.        ]\n",
            "   [-12.49049766   2.73456533   0.          11.45580979  -0.61430857\n",
            "     -0.3708751    0.        ]\n",
            "   [  0.           0.           0.           0.          -1.24790406\n",
            "     18.2349939   -3.51770161]\n",
            "   [  0.           3.90779729  -0.76531717 -22.61823124   4.42963627\n",
            "    -11.83620616   2.5913202 ]\n",
            "   [  0.          -2.57510521   0.56377205  14.9046434   -3.26309825\n",
            "      0.           0.        ]]]\n",
            "\n",
            "\n",
            " [[[  0.           0.           0.          -7.37632211   1.44460562\n",
            "      0.           0.        ]\n",
            "   [  0.           8.9771577   -1.75811906   4.86074483  -1.06417091\n",
            "     -6.33022027   1.23973325]\n",
            "   [  0.          -5.91564092 -20.46169216   4.26093305   2.37001278\n",
            "      3.7072475   -0.91325137]\n",
            "   [ 23.72188694  -4.6457802   14.33699829  -3.13882276  -1.56175764\n",
            "      0.34191818   0.        ]\n",
            "   [-15.63191489   8.31294716  -0.95779793   0.           0.\n",
            "      0.           0.        ]\n",
            "   [  0.          -3.22275593   0.70556329 -28.30681973  28.02297002\n",
            "     -4.40241964   0.        ]\n",
            "   [  0.           0.           0.          18.65322931 -18.89684845\n",
            "      3.24304907   0.        ]]]\n",
            "\n",
            "\n",
            " [[[-10.95054328   2.14459403   0.           0.           0.\n",
            "      0.           0.        ]\n",
            "   [  7.21603475  -1.57981843   0.           8.99780724  -1.76216314\n",
            "      7.72174817  -1.51225511]\n",
            "   [  0.         -28.9365029    5.66702946  -5.92924826   1.29810013\n",
            "     -5.08836884   1.11400501]\n",
            "   [  0.          19.06816905  -4.17462581  26.5394608   -5.19758406\n",
            "     -2.89099606   0.56618313]\n",
            "   [  0.           0.          34.52930929 -24.25094493   3.82880815\n",
            "    -25.51565012   4.953093  ]\n",
            "   [  0.          -5.9656982  -21.5852939    4.98149159   0.\n",
            "     18.06931846  -3.9559458 ]\n",
            "   [  0.           3.93119176  -0.86066232   0.           0.\n",
            "      0.           0.        ]]]\n",
            "\n",
            "\n",
            " [[[  0.           7.53241351  -1.47517513   0.          -5.31146253\n",
            "      1.04021605   0.        ]\n",
            "   [  0.          -4.96360375  -5.10251936   1.21211716   3.50007275\n",
            "     -0.76627672   0.        ]\n",
            "   [  0.           0.           4.07847799  -0.89290794   1.98859337\n",
            "     -0.38945332   0.        ]\n",
            "   [  0.          19.90419103  -3.89810881 -18.25536761   2.26478202\n",
            "      0.28689138   0.        ]\n",
            "   [  4.10355035 -13.91983757   2.87154774  12.02966498  -2.63367446\n",
            "      0.           0.        ]\n",
            "   [ -2.7040998    0.59201304 -23.75124495   4.65152978  18.86154698\n",
            "     -3.69391363   0.        ]\n",
            "   [  0.           0.          15.65126081  -3.42655643 -12.42911653\n",
            "      2.72112705   0.        ]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[  0.        ,   0.        ,  -5.89396339,   1.15429512,\n",
              "            0.        ,   0.        ,   0.        ],\n",
              "         [  7.17309223,  -1.40480435,   3.88392097,  -0.85031324,\n",
              "           -5.05808802,   0.99059427,   0.        ],\n",
              "         [ 14.22787137,  -2.67730476,   0.        , -17.38452561,\n",
              "            6.73775601,  -0.72972276,   0.        ],\n",
              "         [-12.49049766,   2.73456533,   0.        ,  11.45580979,\n",
              "           -0.61430857,  -0.3708751 ,   0.        ],\n",
              "         [  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "           -1.24790406,  18.2349939 ,  -3.51770161],\n",
              "         [  0.        ,   3.90779729,  -0.76531717, -22.61823125,\n",
              "            4.42963627, -11.83620616,   2.5913202 ],\n",
              "         [  0.        ,  -2.57510521,   0.56377204,  14.9046434 ,\n",
              "           -3.26309825,   0.        ,   0.        ]]],\n",
              "\n",
              "\n",
              "       [[[  0.        ,   0.        ,   0.        ,  -7.37632211,\n",
              "            1.44460562,   0.        ,   0.        ],\n",
              "         [  0.        ,   8.9771577 ,  -1.75811906,   4.86074483,\n",
              "           -1.06417091,  -6.33022027,   1.23973325],\n",
              "         [  0.        ,  -5.91564092, -20.46169216,   4.26093305,\n",
              "            2.37001278,   3.7072475 ,  -0.91325137],\n",
              "         [ 23.72188694,  -4.6457802 ,  14.33699829,  -3.13882276,\n",
              "           -1.56175764,   0.34191818,   0.        ],\n",
              "         [-15.63191489,   8.31294716,  -0.95779793,   0.        ,\n",
              "            0.        ,   0.        ,   0.        ],\n",
              "         [  0.        ,  -3.22275593,   0.70556329, -28.30681973,\n",
              "           28.02297002,  -4.40241964,   0.        ],\n",
              "         [  0.        ,   0.        ,   0.        ,  18.65322931,\n",
              "          -18.89684845,   3.24304907,   0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-10.95054328,   2.14459403,   0.        ,   0.        ,\n",
              "            0.        ,   0.        ,   0.        ],\n",
              "         [  7.21603475,  -1.57981843,   0.        ,   8.99780724,\n",
              "           -1.76216314,   7.72174817,  -1.51225511],\n",
              "         [  0.        , -28.9365029 ,   5.66702946,  -5.92924826,\n",
              "            1.29810013,  -5.08836884,   1.11400501],\n",
              "         [  0.        ,  19.06816905,  -4.17462581,  26.5394608 ,\n",
              "           -5.19758406,  -2.89099606,   0.56618313],\n",
              "         [  0.        ,   0.        ,  34.52930929, -24.25094493,\n",
              "            3.82880815, -25.51565012,   4.953093  ],\n",
              "         [  0.        ,  -5.9656982 , -21.58529389,   4.98149158,\n",
              "            0.        ,  18.06931846,  -3.9559458 ],\n",
              "         [  0.        ,   3.93119176,  -0.86066232,   0.        ,\n",
              "            0.        ,   0.        ,   0.        ]]],\n",
              "\n",
              "\n",
              "       [[[  0.        ,   7.53241351,  -1.47517513,   0.        ,\n",
              "           -5.31146253,   1.04021604,   0.        ],\n",
              "         [  0.        ,  -4.96360375,  -5.10251936,   1.21211716,\n",
              "            3.50007275,  -0.76627672,   0.        ],\n",
              "         [  0.        ,   0.        ,   4.07847799,  -0.89290793,\n",
              "            1.98859337,  -0.38945332,   0.        ],\n",
              "         [  0.        ,  19.90419103,  -3.89810881, -18.25536761,\n",
              "            2.26478202,   0.28689138,   0.        ],\n",
              "         [  4.10355035, -13.91983757,   2.87154774,  12.02966498,\n",
              "           -2.63367446,   0.        ,   0.        ],\n",
              "         [ -2.7040998 ,   0.59201304, -23.75124495,   4.65152978,\n",
              "           18.86154698,  -3.69391363,   0.        ],\n",
              "         [  0.        ,   0.        ,  15.65126081,  -3.42655643,\n",
              "          -12.42911653,   2.72112705,   0.        ]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x12p2drz1rkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "n=5\n",
        "lay=FC_Layer(n,1)\n",
        "m=3\n",
        "x=np.random.randn(m,n)\n",
        "y=np.random.randn(m,1)\n",
        "yhat=lay.feedforward(x)\n",
        "loss_grad=2*(yhat-y)\n",
        "lay.grad(x,loss_grad).reshape(-1,n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4vQrAUoaO3N",
        "colab_type": "code",
        "outputId": "d8a052a4-9f87-4f1e-abc6-90614befd981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "loss_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.45294691],\n",
              "       [4.98561353],\n",
              "       [2.94279091]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TLOorWLaf0I",
        "colab_type": "code",
        "outputId": "3f69b864-961a-4005-fe83-0a88e122eea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "ll=lay.W*loss_grad[:,np.newaxis]\n",
        "ll.flatten()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.61928379, 2.18203601, 8.79488324, 1.99502919, 5.1912372 ,\n",
              "       1.17757899])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99kxdVfgagzq",
        "colab_type": "code",
        "outputId": "f163ee52-fe0b-4d06-ff00-f65719416fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lay.W*loss_grad[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.61928379],\n",
              "       [2.18203601]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZm7Suu9ankc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class FC_Layer:\n",
        "  def __init__(self,i_dim,out_dim):\n",
        "    self.W=np.random.randn(i_dim,out_dim)\n",
        "    self.dw=np.zeros(self.W.shape)\n",
        "  def feedforward(self,x):\n",
        "    self.out=np.dot(x,self.W)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    return self.W*loss_grad[:,np.newaxis]\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=12\n",
        "    self.in_ch=2\n",
        "    self.m=10\n",
        "    self.f1=3\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=2\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=2,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "    self.FC1=FC_Layer(cos*cos,1)\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def backpropagate(self,x,y,yhat):\n",
        "    loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def loss(self,y,yhat):\n",
        "    return np.sum(np.square(y-yhat))\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.loss(y,yhat1)-self.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.loss(y,yhat1)-self.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXsdh2WzBhb",
        "colab_type": "code",
        "outputId": "09200efd-2318-487f-d7c2-432b43852517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "np.random.seed(100)\n",
        "nn=Network()\n",
        "img=nn.gen_images()*10\n",
        "y=np.random.randn(img.shape[0],1)\n",
        "yhat=nn.feedforward(img)\n",
        "nn.backpropagate(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "print(nn.conv1.df)\n",
        "nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[-1647226.65336475  -500902.02328258   846200.26792343]\n",
            "   [ -579101.24084628   863026.29740094   492295.56445153]\n",
            "   [  189301.57272224 -1086762.09144228  -737160.67987375]]\n",
            "\n",
            "  [[  938565.71598106  -334080.03171034  1060481.80275155]\n",
            "   [ -211906.06289044  3194256.63685154  1063801.93299418]\n",
            "   [ 1365172.63262884   319152.54777431  1056268.09502134]]]\n",
            "\n",
            "\n",
            " [[[ -417009.75933885  -642843.20982547   328638.7936571 ]\n",
            "   [  481879.54290746  -387757.37856268  -542231.24471764]\n",
            "   [ -318771.32715056   368763.82471907   135132.8771429 ]]\n",
            "\n",
            "  [[  603316.64427842   -82222.70565587   271685.5415042 ]\n",
            "   [  613111.99023078  -793781.28773355   235643.78667596]\n",
            "   [  390771.17595606   390919.57524453   203022.54567157]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-1647226.65335983,  -500902.0232968 ,   846200.267924  ],\n",
              "         [ -579101.24083981,   863026.29740909,   492295.56445032],\n",
              "         [  189301.57271214, -1086762.09143363,  -737160.67988425]],\n",
              "\n",
              "        [[  938565.71597978,  -334080.03170975,  1060481.80275597],\n",
              "         [ -211906.06288612,  3194256.63684495,  1063801.93298683],\n",
              "         [ 1365172.63262533,   319152.54778229,  1056268.09502952]]],\n",
              "\n",
              "\n",
              "       [[[ -417009.75934975,  -642843.20982173,   328638.79366778],\n",
              "         [  481879.5429077 ,  -387757.37856515,  -542231.24471493],\n",
              "         [ -318771.32715657,   368763.82472925,   135132.87714683]],\n",
              "\n",
              "        [[  603316.64427184,   -82222.70565107,   271685.54149568],\n",
              "         [  613111.99022457,  -793781.2877167 ,   235643.78667623],\n",
              "         [  390771.17595822,   390919.57524419,   203022.54566923]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVnPfYsV51NB",
        "colab_type": "code",
        "outputId": "0a411841-acbf-4f81-c67e-344022a458ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nn.conv2.filters.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 2, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNkD9WpBzGxk",
        "colab_type": "code",
        "outputId": "0a6c12df-9d3c-4e86-963a-399208dd7d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "ftft=nn.conv1.filters\n",
        "print(ftft)\n",
        "ftft[0]=0\n",
        "print(ftft)\n",
        "print(nn.conv1.filters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[-1.74976547  0.3426804 ]\n",
            "   [ 1.1530358  -0.25243604]]]]\n",
            "[[[[0. 0.]\n",
            "   [0. 0.]]]]\n",
            "[[[[0. 0.]\n",
            "   [0. 0.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6THLnoZ36en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class FC_Layer:\n",
        "  def __init__(self,i_dim,out_dim):\n",
        "    self.W=np.random.randn(i_dim,out_dim)\n",
        "    self.dw=np.zeros(self.W.shape)\n",
        "  def feedforward(self,x):\n",
        "    self.out=np.dot(x,self.W)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    return self.W*loss_grad[:,np.newaxis]\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=7\n",
        "    self.in_ch=2\n",
        "    self.m=10\n",
        "    self.f1=2\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=1\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=1,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "    #self.FC1=FC_Layer(cos*cos,1)\n",
        "\n",
        "    self.FC1=Neural_Network([cos*cos,4,1],activation='sig')\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def backpropagate(self,x,y,yhat):\n",
        "    loss_grad=self.FC1.loss_grad(y,yhat)\n",
        "    #loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.compute_grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    #nn.compute_grad(x,loss_grad).sum(axis=0)/2\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def loss(self,y,yhat):\n",
        "    return np.sum(np.square(y-yhat))\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFPuIrcJKZbk",
        "colab_type": "code",
        "outputId": "72578178-2f65-4029-d451-a26060accf5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "np.random.seed(100)\n",
        "nn=Network()\n",
        "img=nn.gen_images()*10\n",
        "y=(np.random.randn(img.shape[0],1)>0)/1\n",
        "yhat=nn.feedforward(img)\n",
        "nn.backpropagate(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "print(nn.conv1.df/img.shape[0])\n",
        "nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 1)\n",
            "[[[[-0.00490425 -0.00100493]\n",
            "   [-0.00073612 -0.00194283]]\n",
            "\n",
            "  [[-0.00656018  0.00172411]\n",
            "   [-0.00374737 -0.00178372]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-0.00490425, -0.00100493],\n",
              "         [-0.00073612, -0.00194283]],\n",
              "\n",
              "        [[-0.00656018,  0.00172411],\n",
              "         [-0.00374737, -0.00178372]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1-zohTHDpOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# different activation functions and their derivatives\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  a=x-np.max(x,axis=1,keepdims=True)\n",
        "  s=np.exp(a)\n",
        "  return s/s.sum(axis=1, keepdims=True)\n",
        "def sigmoid_derivative(s):\n",
        "  return s*(1-s)\n",
        "def tanh(s):\n",
        "  return -1+2/(1+np.exp(-2*s))\n",
        "def tanh_derivative( s):\n",
        "  return 1-s*s\n",
        "def relu( s):\n",
        "  s[s<0]=0\n",
        "  return s\n",
        "def relu_derivative( s):\n",
        "  s[s>0]=1\n",
        "  return s\n",
        "\n",
        "class Neural_Network_Layer(object): #single layer of neural network\n",
        "  def __init__(self,i_dim,o_dim,activation='sig'):\n",
        "\n",
        "    self.initialize(i_dim,o_dim)\n",
        "\n",
        "    self.activation=activation\n",
        "\n",
        "  def initialize(self,i_dim,o_dim): #iniitialize data of the layer\n",
        "    self.w=np.random.randn(i_dim+1,o_dim)*1e-1\n",
        "\n",
        "    self.dw=np.zeros(self.w.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.w.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.w.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x): # feed forward input of the layer to output\n",
        "    z=np.dot(np.c_[x,np.ones(len(x))],self.w)\n",
        "    if(self.activation=='sig'):\n",
        "      self.output=sigmoid(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      self.output=tanh(z)\n",
        "    if(self.activation=='relu'):\n",
        "      self.output=relu(z)\n",
        "    if(self.activation=='soft'):\n",
        "      self.output=softmax(z)\n",
        "    return self.output\n",
        "\n",
        "  def derivative(self,z): # return derivative of the layer's output\n",
        "    if(self.activation=='sig'):\n",
        "      return sigmoid_derivative(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      return tanh_derivative(z)\n",
        "    if(self.activation=='relu'):\n",
        "      return relu_derivative(z)\n",
        "\n",
        "  def compute_grad(self,x,loss_grad): # compute gradient using loss_grad of next and return loss_grad for previeous layer\n",
        "    if(self.activation=='soft'):\n",
        "      loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "      self.dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x, np.ones(len(x))])/len(x)\n",
        "    else:\n",
        "      l_jacobian_w=np.einsum('ij,ik->ikj',self.derivative(self.output),np.c_[x, np.ones(len(x))])\n",
        "      self.dw=np.einsum('ki,kji->ji',loss_grad,l_jacobian_w)/len(x)\n",
        "      l_jacobian_input=np.einsum('ij,kj->kij', self.w[:-1],self.derivative(self.output))\n",
        "      loss_grad_x=np.einsum('ij,ikj->ik',loss_grad,l_jacobian_input)\n",
        "    return loss_grad_x\n",
        "\n",
        "  def perturb_weight(self,ep): # \n",
        "    self.w=self.w+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.dw\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.dw)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.w=self.w-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self,layers=[2,3,1],activation='sig',classes=4): #initialize layers and activation function of layers\n",
        "    self.classes=classes\n",
        "    self.layers=[]\n",
        "    self.no_of_layers=len(layers)-1\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==(self.no_of_layers-1)):\n",
        "        self.classes=layers[i+1]\n",
        "        if(self.classes==1):\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1]))  \n",
        "        else:\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],'soft'))\n",
        "      else:        \n",
        "        self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],activation))\n",
        "    for layer in self.layers:\n",
        "      print(layer.w.shape)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==0):\n",
        "        self.layers[i].feedforward(x)\n",
        "      else:\n",
        "        self.layers[i].feedforward(self.layers[i-1].output)\n",
        "    return self.layers[self.no_of_layers-1].output\n",
        "  \n",
        "  def compute_grad(self,x,loss_grad):\n",
        "    grad=loss_grad\n",
        "    for i in range(self.no_of_layers,0,-1):\n",
        "      if((i-1)==0):\n",
        "        grad=self.layers[0].compute_grad(x,grad)\n",
        "      else:\n",
        "        grad=self.layers[i-1].compute_grad(self.layers[i-2].output,grad)\n",
        "\n",
        "    return grad\n",
        "  \n",
        "  \n",
        "  def backpropagate(self,lr,b1=0.9,b2=0.9):\n",
        "    for layer in self.layers: #for each layer update weights\n",
        "      layer.update_weights(lr,b1,b2)\n",
        "\n",
        "  def train(self, trainX, trainY,epochs = 100, learningRate = 0.001,batchSize=10, plot_err = True ,validationX = None, validationY = None):\n",
        "    error=[]\n",
        "    a=[]\n",
        "    t=[]\n",
        "    preL=0\n",
        "    batch_s=np.random.randint(0,len(trainY),batchSize)\n",
        "    for epoch in range(epochs):\n",
        "      batch=(batch_s+epoch)%len(trainY)\n",
        "      #print(batch)\n",
        "      yhat=self.feedforward(trainX[batch])\n",
        "      loss_grad=self.loss_grad(trainY[batch],yhat)\n",
        "      self.compute_grad(trainX[batch],loss_grad)\n",
        "      self.backpropagate(lr=learningRate)\n",
        "      loss=self.loss(trainY[batch],yhat)\n",
        "      error.append(loss)\n",
        "      if(epoch%(epochs/10)==0):\n",
        "        t.append(self.accuracy(trainX,trainY))\n",
        "        if validationX is not None:\n",
        "          a.append(self.accuracy(validationX,validationY))\n",
        "          \n",
        "      if(epoch%(epochs/10)==0):\n",
        "        print(loss)\n",
        "    if(plot_err==True):\n",
        "          plt.figure(1)\n",
        "          plt.plot(np.arange(epochs),np.asarray(error))\n",
        "          plt.figure(2)\n",
        "          plt.plot(np.arange(len(a)),np.asarray(a))\n",
        "          plt.figure(3)\n",
        "          plt.plot(np.arange(len(t)),np.asarray(t))\n",
        "        \n",
        "  def accuracy(self,X,y):\n",
        "    yhat=self.feedforward(X)\n",
        "    ypre=(np.argmax(yhat,axis=1)-y)\n",
        "    acc=np.sum(ypre==0)/len(y)\n",
        "    return acc\n",
        "\n",
        "  def loss(self,y,yhat): # return loss depending upon type of classification\n",
        "    if(self.classes==1):\n",
        "      return self.crossentropy(y,yhat)\n",
        "    else:\n",
        "      return self.MCE(y,yhat)\n",
        "\n",
        "  def loss_grad(self,y,yhat):\n",
        "    if(self.classes==1):\n",
        "      return self.binary_loss_grad(y,yhat).T\n",
        "    else:\n",
        "      return self.MCE_grad(y,yhat)\n",
        "  \n",
        "  def MCE_grad(self,y,yhat):\n",
        "    return yhat-np.eye(self.classes)[y]\n",
        "\n",
        "  def MCE(self, Y, Y_pred):\n",
        "    return -np.sum(np.log(Y_pred[np.eye(self.classes,dtype='bool')[Y]]))/len(Y) \n",
        "\n",
        "  def binary_loss_grad(self,y,yhat):\n",
        "    return -y/yhat.T+(1-y)/(1-yhat.T)\n",
        "\n",
        "  def grad_check_input(self,x,y,grad_desired,in_dim): # check gradient w.r.t input using numerical gradient method\n",
        "    n=in_dim\n",
        "    grad=np.zeros(grad_desired.shape)\n",
        "    ep=np.eye(n,n)*1e-5\n",
        "    for i in range(n):\n",
        "      y1=self.feedforward(x+ep[i])\n",
        "      y2=self.feedforward(x-ep[i])\n",
        "      der=self.loss(y,y1)-self.loss(y,y2) \n",
        "      grad[i]=der/2e-5\n",
        "    print(grad)\n",
        "    return np.linalg.norm(grad-grad_desired)\n",
        "\n",
        "  def grad_check_weights(self,x,y,layer):\n",
        "      #print(self.layers[layer].w)\n",
        "      dw=np.zeros((self.layers[layer].w.shape[0],self.layers[layer].w.shape[1]))\n",
        "      for i in range(self.layers[layer].w.shape[0]):\n",
        "        for j in range(self.layers[layer].w.shape[1]):\n",
        "          ep=np.zeros(self.layers[layer].w.shape)\n",
        "          ep[i,j]=1e-5\n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          y1=self.feedforward(x)\n",
        "          self.layers[layer].perturb_weight(-2*ep)\n",
        "          y2=self.feedforward(x)\n",
        "          der=self.loss(y,y1)-self.loss(y,y2) \n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          dw[i,j]=der/2e-5\n",
        "      print(dw)\n",
        "      return np.linalg.norm(dw-self.layers[layer].dw)\n",
        "\n",
        "  def crossentropy(self, y, y_hat):\n",
        "    loss=-np.sum(y*np.log(y_hat.T)+(1-y)*np.log(1-y_hat.T))/len(y) \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QTCMr7bDqd6",
        "colab_type": "code",
        "outputId": "b83d1cb7-8908-4eb3-982c-494e4319e370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "nn=Neural_Network([4,4,1],activation='sig')\n",
        "x=np.random.randn(2,4)\n",
        "y=np.array([0,1])\n",
        "yhat=nn.feedforward(x)\n",
        "loss_grad=nn.loss_grad(y,yhat)\n",
        "print(loss_grad.shape)\n",
        "gd=nn.compute_grad(x,loss_grad).sum(axis=0)/2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 1)\n",
            "(2, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrxbtdAFZNg",
        "colab_type": "code",
        "outputId": "a41587df-2d12-4633-b1a8-021e18519f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nn.grad_check_input(x,y,gd,4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.64312677e-04 -4.47238424e-04 -1.31450517e-05 -1.70562492e-04]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.4457734244495645e-12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGr_L3XsFowc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  a=x-np.max(x,axis=1,keepdims=True)\n",
        "  s=np.exp(a)\n",
        "  return s/s.sum(axis=1, keepdims=True)\n",
        "def sigmoid_derivative(s):\n",
        "  return s*(1-s)\n",
        "def tanh(s):\n",
        "  return -1+2/(1+np.exp(-2*s))\n",
        "def tanh_derivative( s):\n",
        "  return 1-s*s\n",
        "def relu( s):\n",
        "  s[s<0]=0\n",
        "  return s\n",
        "def relu_derivative( s):\n",
        "  s[s>0]=1\n",
        "  return s\n",
        "\n",
        "class Neural_Network_Layer(object): #single layer of neural network\n",
        "  def __init__(self,i_dim,o_dim,activation='sig'):\n",
        "\n",
        "    self.initialize(i_dim,o_dim)\n",
        "\n",
        "    self.activation=activation\n",
        "\n",
        "  def initialize(self,i_dim,o_dim): #iniitialize data of the layer\n",
        "    self.w=np.random.randn(i_dim+1,o_dim)*1e-1\n",
        "\n",
        "    self.dw=np.zeros(self.w.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.w.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.w.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x): # feed forward input of the layer to output\n",
        "    z=np.dot(np.c_[x,np.ones(len(x))],self.w)\n",
        "    if(self.activation=='sig'):\n",
        "      self.output=sigmoid(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      self.output=tanh(z)\n",
        "    if(self.activation=='relu'):\n",
        "      self.output=relu(z)\n",
        "    if(self.activation=='soft'):\n",
        "      self.output=softmax(z)\n",
        "    return self.output\n",
        "\n",
        "  def derivative(self,z): # return derivative of the layer's output\n",
        "    if(self.activation=='sig'):\n",
        "      return sigmoid_derivative(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      return tanh_derivative(z)\n",
        "    if(self.activation=='relu'):\n",
        "      return relu_derivative(z)\n",
        "\n",
        "  def compute_grad(self,x,loss_grad): # compute gradient using loss_grad of next and return loss_grad for previeous layer\n",
        "    if(self.activation=='soft'):\n",
        "      loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "      self.dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x, np.ones(len(x))])/len(x)\n",
        "    else:\n",
        "      l_jacobian_w=np.einsum('ij,ik->ikj',self.derivative(self.output),np.c_[x, np.ones(len(x))])\n",
        "      self.dw=np.einsum('ki,kji->ji',loss_grad,l_jacobian_w)/len(x)\n",
        "      l_jacobian_input=np.einsum('ij,kj->kij', self.w[:-1],self.derivative(self.output))\n",
        "      loss_grad_x=np.einsum('ij,ikj->ik',loss_grad,l_jacobian_input)\n",
        "    return loss_grad_x\n",
        "\n",
        "  def perturb_weight(self,ep): # \n",
        "    self.w=self.w+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.dw\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.dw)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.w=self.w-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "    self.df=np.zeros(self.filters.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.filters.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.filters.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.df\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.df)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.filters=self.filters-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self,layers=[2,3,1],activation='sig',classes=4): #initialize layers and activation function of layers\n",
        "    self.classes=classes\n",
        "    self.layers=[]\n",
        "    self.no_of_layers=len(layers)-1\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==(self.no_of_layers-1)):\n",
        "        self.classes=layers[i+1]\n",
        "        if(self.classes==1):\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1]))  \n",
        "        else:\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],'soft'))\n",
        "      else:        \n",
        "        self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],activation))\n",
        "    for layer in self.layers:\n",
        "      print(layer.w.shape)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==0):\n",
        "        self.layers[i].feedforward(x)\n",
        "      else:\n",
        "        self.layers[i].feedforward(self.layers[i-1].output)\n",
        "    return self.layers[self.no_of_layers-1].output\n",
        "  \n",
        "  def grad(self,x,loss_grad):\n",
        "    grad=loss_grad\n",
        "    for i in range(self.no_of_layers,0,-1):\n",
        "      if((i-1)==0):\n",
        "        grad=self.layers[0].compute_grad(x,grad)\n",
        "      else:\n",
        "        grad=self.layers[i-1].compute_grad(self.layers[i-2].output,grad)\n",
        "\n",
        "    return grad\n",
        "  \n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    for layer in self.layers: #for each layer update weights\n",
        "      layer.update_weights(lr,b1,b2)\n",
        "\n",
        "  def loss(self,y,yhat): # return loss depending upon type of classification\n",
        "    if(self.classes==1):\n",
        "      return self.crossentropy(y,yhat)\n",
        "    else:\n",
        "      return self.MCE(y,yhat)\n",
        "\n",
        "  def loss_grad(self,y,yhat):\n",
        "    if(self.classes==1):\n",
        "      return self.binary_loss_grad(y,yhat).T\n",
        "    else:\n",
        "      return self.MCE_grad(y,yhat)\n",
        "  \n",
        "  def MCE_grad(self,y,yhat):\n",
        "    return yhat-np.eye(self.classes)[y]\n",
        "\n",
        "  def MCE(self, Y, Y_pred):\n",
        "    return -np.sum(np.log(Y_pred[np.eye(self.classes,dtype='bool')[Y]]))/len(Y) \n",
        "\n",
        "  def binary_loss_grad(self,y,yhat):\n",
        "    return -y/yhat.T+(1-y)/(1-yhat.T)\n",
        "\n",
        "  def grad_check_input(self,x,y,grad_desired,in_dim): # check gradient w.r.t input using numerical gradient method\n",
        "    n=in_dim\n",
        "    grad=np.zeros(grad_desired.shape)\n",
        "    ep=np.eye(n,n)*1e-5\n",
        "    for i in range(n):\n",
        "      y1=self.feedforward(x+ep[i])\n",
        "      y2=self.feedforward(x-ep[i])\n",
        "      der=self.loss(y,y1)-self.loss(y,y2) \n",
        "      grad[i]=der/2e-5\n",
        "    print(grad)\n",
        "    return np.linalg.norm(grad-grad_desired)\n",
        "\n",
        "  def grad_check_weights(self,x,y,layer):\n",
        "      #print(self.layers[layer].w)\n",
        "      dw=np.zeros((self.layers[layer].w.shape[0],self.layers[layer].w.shape[1]))\n",
        "      for i in range(self.layers[layer].w.shape[0]):\n",
        "        for j in range(self.layers[layer].w.shape[1]):\n",
        "          ep=np.zeros(self.layers[layer].w.shape)\n",
        "          ep[i,j]=1e-5\n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          y1=self.feedforward(x)\n",
        "          self.layers[layer].perturb_weight(-2*ep)\n",
        "          y2=self.feedforward(x)\n",
        "          der=self.loss(y,y1)-self.loss(y,y2) \n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          dw[i,j]=der/2e-5\n",
        "      print(dw)\n",
        "      return np.linalg.norm(dw-self.layers[layer].dw)\n",
        "\n",
        "  def crossentropy(self, y, y_hat):\n",
        "    loss=-np.sum(y*np.log(y_hat.T)+(1-y)*np.log(1-y_hat.T))/len(y) \n",
        "    return loss\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=7\n",
        "    self.in_ch=2\n",
        "    self.m=10\n",
        "    self.f1=2\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=2\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=2,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "\n",
        "    self.FC1=Neural_Network([cos*cos,4,1],activation='sig')\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.conv1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.conv2.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.FC1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def grad(self,x,y,yhat):\n",
        "    loss_grad=self.FC1.loss_grad(y,yhat)\n",
        "    #loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    #nn.compute_grad(x,loss_grad).sum(axis=0)/2\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACj1g5rZYqOw",
        "colab_type": "code",
        "outputId": "2db464a7-6465-4470-e19c-6be18e210f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 449
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuPf7FG4Rmsj",
        "colab_type": "code",
        "outputId": "148ac6ce-7629-483e-a082-beb52a3bac9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "np.random.seed(100000)\n",
        "nn=Network()\n",
        "img=nn.gen_images()*10\n",
        "y=(np.random.randn(img.shape[0],1)>0)/1\n",
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "print(nn.conv2.df/img.shape[0])\n",
        "nn.num_grad_df(img,y,nn.conv2.filters.shape,nn.conv2.perturb_f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 1)\n",
            "[[[[-0.00396585 -0.00151968]\n",
            "   [ 0.01451818  0.01770437]]\n",
            "\n",
            "  [[ 0.00575853  0.01763064]\n",
            "   [ 0.0108141   0.01422644]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-0.00396585, -0.00151968],\n",
              "         [ 0.01451818,  0.01770437]],\n",
              "\n",
              "        [[ 0.00575853,  0.01763064],\n",
              "         [ 0.0108141 ,  0.01422644]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 450
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U3M-dfNYZbv",
        "colab_type": "code",
        "outputId": "365b109e-05f0-4586-b871-033343234f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "epochs=100\n",
        "for epoch in range(epochs):\n",
        "  yhat=nn.feedforward(img)\n",
        "  nn.grad(img,y,yhat)\n",
        "  nn.update_weights(1e-1)\n",
        "  if(epoch%(epochs/10)==1):\n",
        "    print(nn.FC1.loss(y,yhat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.5109335251232325\n",
            "0.9179436942021401\n",
            "0.05955839335191915\n",
            "0.00819067528497561\n",
            "0.001555716525888575\n",
            "0.0004954120635887958\n",
            "0.0003068551790740968\n",
            "0.00022531671026705034\n",
            "0.00014062657180046858\n",
            "0.00012023857693523599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrZqziD3ZK2g",
        "colab_type": "code",
        "outputId": "9d02530c-989d-4e87-f7c2-80eeff90c638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.randint(10,size=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 8, 1, 1, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2wCKQIgw2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  a=x-np.max(x,axis=1,keepdims=True)\n",
        "  s=np.exp(a)\n",
        "  return s/s.sum(axis=1, keepdims=True)\n",
        "def sigmoid_derivative(s):\n",
        "  return s*(1-s)\n",
        "def tanh(s):\n",
        "  return -1+2/(1+np.exp(-2*s))\n",
        "def tanh_derivative( s):\n",
        "  return 1-s*s\n",
        "def relu( s):\n",
        "  s[s<0]=0\n",
        "  return s\n",
        "def relu_derivative( s):\n",
        "  s[s>0]=1\n",
        "  return s\n",
        "\n",
        "class Neural_Network_Layer(object): #single layer of neural network\n",
        "  def __init__(self,i_dim,o_dim,activation='sig'):\n",
        "\n",
        "    self.initialize(i_dim,o_dim)\n",
        "\n",
        "    self.activation=activation\n",
        "\n",
        "  def initialize(self,i_dim,o_dim): #iniitialize data of the layer\n",
        "    self.w=np.random.randn(i_dim+1,o_dim)*1e-1\n",
        "\n",
        "    self.dw=np.zeros(self.w.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.w.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.w.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x): # feed forward input of the layer to output\n",
        "    z=np.dot(np.c_[x,np.ones(len(x))],self.w)\n",
        "    if(self.activation=='sig'):\n",
        "      self.output=sigmoid(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      self.output=tanh(z)\n",
        "    if(self.activation=='relu'):\n",
        "      self.output=relu(z)\n",
        "    if(self.activation=='soft'):\n",
        "      self.output=softmax(z)\n",
        "    return self.output\n",
        "\n",
        "  def derivative(self,z): # return derivative of the layer's output\n",
        "    if(self.activation=='sig'):\n",
        "      return sigmoid_derivative(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      return tanh_derivative(z)\n",
        "    if(self.activation=='relu'):\n",
        "      return relu_derivative(z)\n",
        "\n",
        "  def compute_grad(self,x,loss_grad): # compute gradient using loss_grad of next and return loss_grad for previeous layer\n",
        "    if(self.activation=='soft'):\n",
        "      loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "      self.dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x, np.ones(len(x))])/len(x)\n",
        "    else:\n",
        "      l_jacobian_w=np.einsum('ij,ik->ikj',self.derivative(self.output),np.c_[x, np.ones(len(x))])\n",
        "      self.dw=np.einsum('ki,kji->ji',loss_grad,l_jacobian_w)/len(x)\n",
        "      l_jacobian_input=np.einsum('ij,kj->kij', self.w[:-1],self.derivative(self.output))\n",
        "      loss_grad_x=np.einsum('ij,ikj->ik',loss_grad,l_jacobian_input)\n",
        "    return loss_grad_x\n",
        "\n",
        "  def perturb_weight(self,ep): # \n",
        "    self.w=self.w+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.dw\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.dw)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.w=self.w-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "    self.df=np.zeros(self.filters.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.filters.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.filters.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.df\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.df)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.filters=self.filters-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self,layers=[2,3,1],activation='sig',classes=4): #initialize layers and activation function of layers\n",
        "    self.classes=classes\n",
        "    self.layers=[]\n",
        "    self.no_of_layers=len(layers)-1\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==(self.no_of_layers-1)):\n",
        "        self.classes=layers[i+1]\n",
        "        if(self.classes==1):\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1]))  \n",
        "        else:\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],'soft'))\n",
        "      else:        \n",
        "        self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],activation))\n",
        "    for layer in self.layers:\n",
        "      print(layer.w.shape)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==0):\n",
        "        self.layers[i].feedforward(x)\n",
        "      else:\n",
        "        self.layers[i].feedforward(self.layers[i-1].output)\n",
        "    return self.layers[self.no_of_layers-1].output\n",
        "  \n",
        "  def grad(self,x,loss_grad):\n",
        "    grad=loss_grad\n",
        "    for i in range(self.no_of_layers,0,-1):\n",
        "      if((i-1)==0):\n",
        "        grad=self.layers[0].compute_grad(x,grad)\n",
        "      else:\n",
        "        grad=self.layers[i-1].compute_grad(self.layers[i-2].output,grad)\n",
        "\n",
        "    return grad\n",
        "  \n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    for layer in self.layers: #for each layer update weights\n",
        "      layer.update_weights(lr,b1,b2)\n",
        "\n",
        "  def loss(self,y,yhat): # return loss depending upon type of classification\n",
        "    if(self.classes==1):\n",
        "      return self.crossentropy(y,yhat)\n",
        "    else:\n",
        "      return self.MCE(y,yhat)\n",
        "\n",
        "  def loss_grad(self,y,yhat):\n",
        "    if(self.classes==1):\n",
        "      return self.binary_loss_grad(y,yhat).T\n",
        "    else:\n",
        "      return self.MCE_grad(y,yhat)\n",
        "  \n",
        "  def MCE_grad(self,y,yhat):\n",
        "    return yhat-np.eye(self.classes)[y]\n",
        "\n",
        "  def MCE(self, Y, Y_pred):\n",
        "    return -np.sum(np.log(Y_pred[np.eye(self.classes,dtype='bool')[Y]]))/len(Y) \n",
        "\n",
        "  def binary_loss_grad(self,y,yhat):\n",
        "    return -y/yhat.T+(1-y)/(1-yhat.T)\n",
        "\n",
        "  def grad_check_input(self,x,y,grad_desired,in_dim): # check gradient w.r.t input using numerical gradient method\n",
        "    n=in_dim\n",
        "    grad=np.zeros(grad_desired.shape)\n",
        "    ep=np.eye(n,n)*1e-5\n",
        "    for i in range(n):\n",
        "      y1=self.feedforward(x+ep[i])\n",
        "      y2=self.feedforward(x-ep[i])\n",
        "      der=self.loss(y,y1)-self.loss(y,y2) \n",
        "      grad[i]=der/2e-5\n",
        "    print(grad)\n",
        "    return np.linalg.norm(grad-grad_desired)\n",
        "\n",
        "  def grad_check_weights(self,x,y,layer):\n",
        "      #print(self.layers[layer].w)\n",
        "      dw=np.zeros((self.layers[layer].w.shape[0],self.layers[layer].w.shape[1]))\n",
        "      for i in range(self.layers[layer].w.shape[0]):\n",
        "        for j in range(self.layers[layer].w.shape[1]):\n",
        "          ep=np.zeros(self.layers[layer].w.shape)\n",
        "          ep[i,j]=1e-5\n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          y1=self.feedforward(x)\n",
        "          self.layers[layer].perturb_weight(-2*ep)\n",
        "          y2=self.feedforward(x)\n",
        "          der=self.loss(y,y1)-self.loss(y,y2) \n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          dw[i,j]=der/2e-5\n",
        "      print(dw)\n",
        "      return np.linalg.norm(dw-self.layers[layer].dw)\n",
        "\n",
        "  def crossentropy(self, y, y_hat):\n",
        "    loss=-np.sum(y*np.log(y_hat.T)+(1-y)*np.log(1-y_hat.T))/len(y) \n",
        "    return loss\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=7\n",
        "    self.in_ch=2\n",
        "    self.m=10\n",
        "    self.f1=2\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=2\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=2,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "\n",
        "    self.FC1=Neural_Network([cos*cos,4,10],activation='sig')\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.conv1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.conv2.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.FC1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def grad(self,x,y,yhat):\n",
        "    loss_grad=self.FC1.loss_grad(y,yhat)\n",
        "    #loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    #nn.compute_grad(x,loss_grad).sum(axis=0)/2\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmRP1bgyhOdR",
        "colab_type": "code",
        "outputId": "074cc7e3-9270-40ad-9749-8f6294db7483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "np.random.seed(100)\n",
        "nn=Network()\n",
        "img=nn.gen_images()*10\n",
        "y=np.random.randint(10,size=img.shape[0])\n",
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "print(nn.conv1.df/img.shape[0])\n",
        "nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 10)\n",
            "[[[[ 0.00770654 -0.03935922]\n",
            "   [ 0.02430351  0.01140026]]\n",
            "\n",
            "  [[ 0.00142065 -0.03883893]\n",
            "   [-0.00033664 -0.02738983]]]\n",
            "\n",
            "\n",
            " [[[ 0.03897154  0.01716497]\n",
            "   [-0.008745    0.01168953]]\n",
            "\n",
            "  [[-0.0621539  -0.0059106 ]\n",
            "   [-0.04222764 -0.01230967]]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 0.00770654, -0.03935922],\n",
              "         [ 0.02430351,  0.01140026]],\n",
              "\n",
              "        [[ 0.00142065, -0.03883893],\n",
              "         [-0.00033664, -0.02738983]]],\n",
              "\n",
              "\n",
              "       [[[ 0.03897154,  0.01716497],\n",
              "         [-0.008745  ,  0.01168953]],\n",
              "\n",
              "        [[-0.0621539 , -0.0059106 ],\n",
              "         [-0.04222764, -0.01230967]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X3DN8pLh7yZ",
        "colab_type": "code",
        "outputId": "32ff08ea-eb5b-4b7e-e8c8-3a16bce41d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "epochs=1000\n",
        "for epoch in range(epochs):\n",
        "  yhat=nn.feedforward(img)\n",
        "  nn.grad(img,y,yhat)\n",
        "  nn.update_weights(1e-1)\n",
        "  if(epoch%(epochs/10)==1):\n",
        "    print(nn.FC1.loss(y,yhat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.236857274327835\n",
            "0.44846150968029647\n",
            "0.4041807417658849\n",
            "0.14679373125130996\n",
            "0.13875747908892475\n",
            "0.13863397198028793\n",
            "0.13863151501936904\n",
            "4.537658267221929\n",
            "4.468635830806367\n",
            "5.146412883492092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJS4sjMJk_W_",
        "colab_type": "code",
        "outputId": "2ec5843b-b1f0-4983-a8e7-a0df408a530a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "def sigmoid(x):\n",
        "  print(x.shape)\n",
        "  x[x>30]=30\n",
        "  x[x<-30]=-30\n",
        "  return 1/(1+np.exp(-x))\n",
        "y=np.random.randn(4,4)*30\n",
        "sigmoid(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.02128503e-09, 9.99998806e-01, 7.09702027e-05, 9.35762297e-14],\n",
              "       [9.99999092e-01, 9.35762297e-14, 3.79084304e-01, 1.00000000e+00],\n",
              "       [2.31054578e-11, 1.00000000e+00, 4.64463301e-05, 9.35762297e-14],\n",
              "       [2.45080668e-11, 4.40727662e-02, 1.85832942e-07, 1.73799407e-10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk-6rQ4Nittk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  #x[x>30]=30\n",
        "  #x[x<-30]=-30\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  a=x-np.max(x,axis=1,keepdims=True)\n",
        "  s=np.exp(a)\n",
        "  return s/s.sum(axis=1, keepdims=True)\n",
        "def sigmoid_derivative(s):\n",
        "  return s*(1-s)\n",
        "def tanh(s):\n",
        "  return -1+2/(1+np.exp(-2*s))\n",
        "def tanh_derivative( s):\n",
        "  return 1-s*s\n",
        "def relu( s):\n",
        "  s[s<0]=0\n",
        "  return s\n",
        "def relu_derivative( s):\n",
        "  s[s>0]=1\n",
        "  return s\n",
        "\n",
        "class Neural_Network_Layer(object): #single layer of neural network\n",
        "  def __init__(self,i_dim,o_dim,activation='sig'):\n",
        "\n",
        "    self.initialize(i_dim,o_dim)\n",
        "\n",
        "    self.activation=activation\n",
        "\n",
        "  def initialize(self,i_dim,o_dim): #iniitialize data of the layer\n",
        "    self.w=np.random.randn(i_dim+1,o_dim)*1e-1\n",
        "\n",
        "    self.dw=np.zeros(self.w.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.w.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.w.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x): # feed forward input of the layer to output\n",
        "    z=np.dot(np.c_[x,np.ones(len(x))],self.w)\n",
        "    if(self.activation=='sig'):\n",
        "      self.output=sigmoid(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      self.output=tanh(z)\n",
        "    if(self.activation=='relu'):\n",
        "      self.output=relu(z)\n",
        "    if(self.activation=='soft'):\n",
        "      self.output=softmax(z)\n",
        "    return self.output\n",
        "\n",
        "  def derivative(self,z): # return derivative of the layer's output\n",
        "    if(self.activation=='sig'):\n",
        "      return sigmoid_derivative(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      return tanh_derivative(z)\n",
        "    if(self.activation=='relu'):\n",
        "      return relu_derivative(z)\n",
        "\n",
        "  def compute_grad(self,x,loss_grad): # compute gradient using loss_grad of next and return loss_grad for previeous layer\n",
        "    if(self.activation=='soft'):\n",
        "      loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "      self.dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x, np.ones(len(x))])/len(x)\n",
        "    else:\n",
        "      l_jacobian_w=np.einsum('ij,ik->ikj',self.derivative(self.output),np.c_[x, np.ones(len(x))])\n",
        "      self.dw=np.einsum('ki,kji->ji',loss_grad,l_jacobian_w)/len(x)\n",
        "      l_jacobian_input=np.einsum('ij,kj->kij', self.w[:-1],self.derivative(self.output))\n",
        "      loss_grad_x=np.einsum('ij,ikj->ik',loss_grad,l_jacobian_input)\n",
        "    return loss_grad_x\n",
        "\n",
        "  def perturb_weight(self,ep): # \n",
        "    self.w=self.w+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.dw\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.dw)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.w=self.w-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "    self.df=np.zeros(self.filters.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.filters.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.filters.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(1, 1),(1,1)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.df\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.df)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.filters=self.filters-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self,layers=[2,3,1],activation='sig',classes=4): #initialize layers and activation function of layers\n",
        "    self.classes=classes\n",
        "    self.layers=[]\n",
        "    self.no_of_layers=len(layers)-1\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==(self.no_of_layers-1)):\n",
        "        self.classes=layers[i+1]\n",
        "        if(self.classes==1):\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1]))  \n",
        "        else:\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],'soft'))\n",
        "      else:        \n",
        "        self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],activation))\n",
        "    for layer in self.layers:\n",
        "      print(layer.w.shape)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==0):\n",
        "        self.layers[i].feedforward(x)\n",
        "      else:\n",
        "        self.layers[i].feedforward(self.layers[i-1].output)\n",
        "    return self.layers[self.no_of_layers-1].output\n",
        "  \n",
        "  def grad(self,x,loss_grad):\n",
        "    grad=loss_grad\n",
        "    for i in range(self.no_of_layers,0,-1):\n",
        "      if((i-1)==0):\n",
        "        grad=self.layers[0].compute_grad(x,grad)\n",
        "      else:\n",
        "        grad=self.layers[i-1].compute_grad(self.layers[i-2].output,grad)\n",
        "\n",
        "    return grad\n",
        "  \n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    for layer in self.layers: #for each layer update weights\n",
        "      layer.update_weights(lr,b1,b2)\n",
        "\n",
        "  def loss(self,y,yhat): # return loss depending upon type of classification\n",
        "    if(self.classes==1):\n",
        "      return self.crossentropy(y,yhat)\n",
        "    else:\n",
        "      return self.MCE(y,yhat)\n",
        "\n",
        "  def loss_grad(self,y,yhat):\n",
        "    if(self.classes==1):\n",
        "      return self.binary_loss_grad(y,yhat).T\n",
        "    else:\n",
        "      return self.MCE_grad(y,yhat)\n",
        "  \n",
        "  def MCE_grad(self,y,yhat):\n",
        "    return yhat-np.eye(self.classes)[y]\n",
        "\n",
        "  def MCE(self, Y, Y_pred):\n",
        "    return -np.sum(np.log(Y_pred[np.eye(self.classes,dtype='bool')[Y]]))/len(Y) \n",
        "\n",
        "  def binary_loss_grad(self,y,yhat):\n",
        "    return -y/yhat.T+(1-y)/(1-yhat.T)\n",
        "\n",
        "  def grad_check_input(self,x,y,grad_desired,in_dim): # check gradient w.r.t input using numerical gradient method\n",
        "    n=in_dim\n",
        "    grad=np.zeros(grad_desired.shape)\n",
        "    ep=np.eye(n,n)*1e-5\n",
        "    for i in range(n):\n",
        "      y1=self.feedforward(x+ep[i])\n",
        "      y2=self.feedforward(x-ep[i])\n",
        "      der=self.loss(y,y1)-self.loss(y,y2) \n",
        "      grad[i]=der/2e-5\n",
        "    print(grad)\n",
        "    return np.linalg.norm(grad-grad_desired)\n",
        "\n",
        "  def grad_check_weights(self,x,y,layer):\n",
        "      #print(self.layers[layer].w)\n",
        "      dw=np.zeros((self.layers[layer].w.shape[0],self.layers[layer].w.shape[1]))\n",
        "      for i in range(self.layers[layer].w.shape[0]):\n",
        "        for j in range(self.layers[layer].w.shape[1]):\n",
        "          ep=np.zeros(self.layers[layer].w.shape)\n",
        "          ep[i,j]=1e-5\n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          y1=self.feedforward(x)\n",
        "          self.layers[layer].perturb_weight(-2*ep)\n",
        "          y2=self.feedforward(x)\n",
        "          der=self.loss(y,y1)-self.loss(y,y2) \n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          dw[i,j]=der/2e-5\n",
        "      print(dw)\n",
        "      return np.linalg.norm(dw-self.layers[layer].dw)\n",
        "\n",
        "  def crossentropy(self, y, y_hat):\n",
        "    loss=-np.sum(y*np.log(y_hat.T)+(1-y)*np.log(1-y_hat.T))/len(y) \n",
        "    return loss\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=7\n",
        "    self.in_ch=2\n",
        "    self.m=10\n",
        "    self.f1=2\n",
        "    self.f2=2\n",
        "    self.cs=1\n",
        "    self.o_ch=2\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=2,out_ch=1,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "\n",
        "    self.FC1=Neural_Network([cos*cos,4,10],activation='sig')\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.conv1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.conv2.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.FC1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def grad(self,x,y,yhat):\n",
        "    loss_grad=self.FC1.loss_grad(y,yhat)\n",
        "    #loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    #nn.compute_grad(x,loss_grad).sum(axis=0)/2\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soT3ZPC1kU5w",
        "colab_type": "code",
        "outputId": "85c91125-f2c9-4c25-df7e-26505938009e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#np.random.seed(10000)\n",
        "nn=Network()\n",
        "img=nn.gen_images()\n",
        "y=np.random.randint(10,size=img.shape[0])\n",
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "#print(nn.conv1.df/img.shape[0])\n",
        "#nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBal2nsdkVSw",
        "colab_type": "code",
        "outputId": "516b308c-837a-47d1-f95e-81da397e0044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "#np.random.seed(10000)\n",
        "nn=Network()\n",
        "img=nn.gen_images()\n",
        "y=np.random.randint(10,size=img.shape[0])\n",
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "#print(nn.conv1.df/img.shape[0])\n",
        "#nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f)\n",
        "epochs=1000\n",
        "l=nn.FC1.loss(y,yhat)\n",
        "for epoch in range(epochs):\n",
        "  yhat=nn.feedforward(img)\n",
        "  nn.grad(img,y,yhat)\n",
        "  nn.update_weights(1e-1)\n",
        "  if(epoch%(epochs/10)==1):\n",
        "    print(nn.FC1.loss(y,yhat))\n",
        "print(nn.conv1.df/img.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 10)\n",
            "2.2911360681367587\n",
            "0.4702339560156881\n",
            "0.46831380552907104\n",
            "0.46822045694782694\n",
            "0.4682188305993259\n",
            "0.4682322216348238\n",
            "0.468219487806483\n",
            "0.468241786335886\n",
            "0.4682296372303306\n",
            "0.46822925256244796\n",
            "[[[[-1.17329919e-14 -9.37534999e-15]\n",
            "   [ 1.00868111e-14 -4.62805757e-15]]\n",
            "\n",
            "  [[-4.38041172e-15  6.91842028e-15]\n",
            "   [ 2.49829617e-14  1.12033241e-14]]]\n",
            "\n",
            "\n",
            " [[[ 4.63650764e-15  1.39992285e-14]\n",
            "   [-1.33687330e-14 -2.98414977e-15]]\n",
            "\n",
            "  [[ 8.47806586e-16  4.18008980e-15]\n",
            "   [ 2.64819224e-15 -1.11313504e-14]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9o7jUxAn5ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Include libraries which may use in implementation\n",
        "import random\n",
        "import sklearn.datasets as ds\n",
        "%matplotlib inline\n",
        "import joblib\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5bGjOy31Kn",
        "colab_type": "code",
        "outputId": "91365073-8800-43d3-99f4-6ecea4b7c7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#import data from gdrive to colab and extract in tmp folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"drive/My Drive/Assignment 2.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"tmp/mydata\")\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"tmp/mydata/Assignment 2/Task3_Data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"tmp/mydata/Assignment 2\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVly6jQp32lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "import glob2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import numpy as np\n",
        "def loadDataset(path):\n",
        "  print('Loading Dataset...')\n",
        "  train_x, train_y, test_x, test_y = [], [], [], []\n",
        "  for i in range(10):\n",
        "    for filename in glob2.glob(path + '/train/' + str(i)+'/*.png'):\n",
        "      im=img.imread(filename)\n",
        "      train_x.append(im)\n",
        "      train_y.append(i)\n",
        "  for i in range(10):\n",
        "    for filename in glob2.glob(path + '/test/' + str(i)+'/*.png'):\n",
        "      im=img.imread(filename)\n",
        "      test_x.append(im)\n",
        "      test_y.append(i)\n",
        "  print('Dataset loaded...')\n",
        "  return np.array(train_x), np.array(train_y), np.array(test_x),np.array(test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qIXt6af4BFd",
        "colab_type": "code",
        "outputId": "66463dd5-e555-4f41-e63c-e1c4c2cde2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_x,train_y,test_x,test_y=loadDataset(\"tmp/mydata/Assignment 2\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Dataset...\n",
            "Dataset loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKsjgv1F4KiC",
        "colab_type": "code",
        "outputId": "1aefb65d-6fcb-4a35-e51b-2e3d30731994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(train_x[25])\n",
        "print(train_x[25].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzvM9k3Z4V_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  a=x-np.max(x,axis=1,keepdims=True)\n",
        "  s=np.exp(a)\n",
        "  return s/s.sum(axis=1, keepdims=True)\n",
        "def sigmoid_derivative(s):\n",
        "  return s*(1-s)\n",
        "def tanh(s):\n",
        "  return -1+2/(1+np.exp(-2*s))\n",
        "def tanh_derivative( s):\n",
        "  return 1-s*s\n",
        "def relu( s):\n",
        "  s[s<0]=0\n",
        "  return s\n",
        "def relu_derivative( s):\n",
        "  s[s>0]=1\n",
        "  return s\n",
        "\n",
        "class Neural_Network_Layer(object): #single layer of neural network\n",
        "  def __init__(self,i_dim,o_dim,activation='sig'):\n",
        "\n",
        "    self.initialize(i_dim,o_dim)\n",
        "\n",
        "    self.activation=activation\n",
        "\n",
        "  def initialize(self,i_dim,o_dim): #iniitialize data of the layer\n",
        "    self.w=np.random.randn(i_dim+1,o_dim)*1e-1\n",
        "\n",
        "    self.dw=np.zeros(self.w.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.w.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.w.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x): # feed forward input of the layer to output\n",
        "    z=np.dot(np.c_[x,np.ones(len(x))],self.w)\n",
        "    #print(x.shape)\n",
        "    #print(z.shape)\n",
        "    if(self.activation=='sig'):\n",
        "      self.output=sigmoid(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      self.output=tanh(z)\n",
        "    if(self.activation=='relu'):\n",
        "      self.output=relu(z)\n",
        "    if(self.activation=='soft'):\n",
        "      self.output=softmax(z)\n",
        "    return self.output\n",
        "\n",
        "  def derivative(self,z): # return derivative of the layer's output\n",
        "    if(self.activation=='sig'):\n",
        "      return sigmoid_derivative(z)\n",
        "    if(self.activation=='tanh'):\n",
        "      return tanh_derivative(z)\n",
        "    if(self.activation=='relu'):\n",
        "      return relu_derivative(z)\n",
        "\n",
        "  def compute_grad(self,x,loss_grad): # compute gradient using loss_grad of next and return loss_grad for previeous layer\n",
        "    if(self.activation=='soft'):\n",
        "      loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "      self.dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x, np.ones(len(x))])/len(x)\n",
        "    else:\n",
        "      l_jacobian_w=np.einsum('ij,ik->ikj',self.derivative(self.output),np.c_[x, np.ones(len(x))])\n",
        "      self.dw=np.einsum('ki,kji->ji',loss_grad,l_jacobian_w)/len(x)\n",
        "      l_jacobian_input=np.einsum('ij,kj->kij', self.w[:-1],self.derivative(self.output))\n",
        "      loss_grad_x=np.einsum('ij,ikj->ik',loss_grad,l_jacobian_input)\n",
        "    return loss_grad_x\n",
        "\n",
        "  def perturb_weight(self,ep): # \n",
        "    self.w=self.w+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.dw\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.dw)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.w=self.w-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "class ConvLayer:\n",
        "  def __init__(self,in_ch=1,out_ch=1,kernel=(2,2),stride=1):\n",
        "    self.filters=np.random.randn(out_ch,in_ch,kernel[0],kernel[1])\n",
        "    self.stride=stride\n",
        "\n",
        "    self.df=np.zeros(self.filters.shape) # differential to be added to w\n",
        "\n",
        "    self.m=np.zeros(self.filters.shape)  # momentum factor in adam update\n",
        "\n",
        "    self.s=np.zeros(self.filters.shape) # learning rate factor in adam update\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    if(x.shape[1]!=self.filters.shape[1]):\n",
        "      print(x.shape[1])\n",
        "      print(self.filters.shape[1])\n",
        "      raise Exception(\"channels in input and output are not same\")\n",
        "    self.out=strided_convolution3D(x,self.filters,self.stride)\n",
        "    return self.out\n",
        "  def grad(self,x,loss_grad):\n",
        "    self.df=strided_convolution3D_grad1(x,loss_grad,1).sum(axis=0)/loss_grad.shape[0]\n",
        "    gg=np.rot90(loss_grad,2,axes=(2,3))\n",
        "    p=self.filters.shape[3]-1\n",
        "    gg1=np.pad(gg, ((0, 0),(0, 0),(p, p),(p,p)), 'constant', constant_values=(0))\n",
        "    ooo=strided_convolution3D1_g(gg1,self.filters,1)\n",
        "    return np.rot90(ooo,2,axes=(3,4)).sum(axis=1)\n",
        "\n",
        "  def perturb_f(self,ep):\n",
        "    self.filters=self.filters+ep\n",
        "\n",
        "  def update_mom(self,b):   \n",
        "    self.m=b*self.m+(1-b)*self.df\n",
        "    \n",
        "  def update_lr(self,b):\n",
        "    self.s=b*self.s+(1-b)*np.square(self.df)\n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.update_mom(b1)\n",
        "    self.update_lr(b2)\n",
        "    self.filters=self.filters-lr*self.m/(np.sqrt(self.s)+1e-8)\n",
        "\n",
        "\n",
        "class Max_Pool_Layer:\n",
        "  def __init__(self,stride):\n",
        "    self.stride=stride\n",
        "  def feedforward(self,x):\n",
        "    self.m_indices=mx_pool(x,self.stride)\n",
        "    self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n",
        "                                               int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
        "    return self.m\n",
        "  def grad(self,x,grad_loss):\n",
        "    grad_zeros=np.zeros(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])\n",
        "    grad_zeros[self.m_indices]=grad_loss.flatten()\n",
        "    gg=grad_zeros.reshape(x.shape)\n",
        "    return gg\n",
        "\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self,layers=[2,3,1],activation='sig',classes=4): #initialize layers and activation function of layers\n",
        "    self.classes=classes\n",
        "    self.layers=[]\n",
        "    self.no_of_layers=len(layers)-1\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==(self.no_of_layers-1)):\n",
        "        self.classes=layers[i+1]\n",
        "        if(self.classes==1):\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1]))  \n",
        "        else:\n",
        "          self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],'soft'))\n",
        "      else:        \n",
        "        self.layers.append(Neural_Network_Layer(layers[i],layers[i+1],activation))\n",
        "    for layer in self.layers:\n",
        "      print(layer.w.shape)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    for i in range(self.no_of_layers):\n",
        "      if(i==0):\n",
        "        self.layers[i].feedforward(x)\n",
        "      else:\n",
        "        self.layers[i].feedforward(self.layers[i-1].output)\n",
        "    return self.layers[self.no_of_layers-1].output\n",
        "  \n",
        "  def grad(self,x,loss_grad):\n",
        "    grad=loss_grad\n",
        "    for i in range(self.no_of_layers,0,-1):\n",
        "      if((i-1)==0):\n",
        "        grad=self.layers[0].compute_grad(x,grad)\n",
        "      else:\n",
        "        grad=self.layers[i-1].compute_grad(self.layers[i-2].output,grad)\n",
        "\n",
        "    return grad\n",
        "  \n",
        "  \n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    for layer in self.layers: #for each layer update weights\n",
        "      layer.update_weights(lr,b1,b2)\n",
        "\n",
        "  def loss(self,y,yhat): # return loss depending upon type of classification\n",
        "    if(self.classes==1):\n",
        "      return self.crossentropy(y,yhat)\n",
        "    else:\n",
        "      return self.MCE(y,yhat)\n",
        "\n",
        "  def loss_grad(self,y,yhat):\n",
        "    if(self.classes==1):\n",
        "      return self.binary_loss_grad(y,yhat).T\n",
        "    else:\n",
        "      return self.MCE_grad(y,yhat)\n",
        "  \n",
        "  def MCE_grad(self,y,yhat):\n",
        "    return yhat-np.eye(self.classes)[y]\n",
        "\n",
        "  def MCE(self, Y, Y_pred):\n",
        "    return -np.sum(np.log(Y_pred[np.eye(self.classes,dtype='bool')[Y]]))/len(Y) \n",
        "\n",
        "  def binary_loss_grad(self,y,yhat):\n",
        "    return -y/yhat.T+(1-y)/(1-yhat.T)\n",
        "\n",
        "  def grad_check_input(self,x,y,grad_desired,in_dim): # check gradient w.r.t input using numerical gradient method\n",
        "    n=in_dim\n",
        "    grad=np.zeros(grad_desired.shape)\n",
        "    ep=np.eye(n,n)*1e-5\n",
        "    for i in range(n):\n",
        "      y1=self.feedforward(x+ep[i])\n",
        "      y2=self.feedforward(x-ep[i])\n",
        "      der=self.loss(y,y1)-self.loss(y,y2) \n",
        "      grad[i]=der/2e-5\n",
        "    print(grad)\n",
        "    return np.linalg.norm(grad-grad_desired)\n",
        "\n",
        "  def grad_check_weights(self,x,y,layer):\n",
        "      #print(self.layers[layer].w)\n",
        "      dw=np.zeros((self.layers[layer].w.shape[0],self.layers[layer].w.shape[1]))\n",
        "      for i in range(self.layers[layer].w.shape[0]):\n",
        "        for j in range(self.layers[layer].w.shape[1]):\n",
        "          ep=np.zeros(self.layers[layer].w.shape)\n",
        "          ep[i,j]=1e-5\n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          y1=self.feedforward(x)\n",
        "          self.layers[layer].perturb_weight(-2*ep)\n",
        "          y2=self.feedforward(x)\n",
        "          der=self.loss(y,y1)-self.loss(y,y2) \n",
        "          self.layers[layer].perturb_weight(ep)\n",
        "          dw[i,j]=der/2e-5\n",
        "      print(dw)\n",
        "      return np.linalg.norm(dw-self.layers[layer].dw)\n",
        "\n",
        "  def crossentropy(self, y, y_hat):\n",
        "    loss=-np.sum(y*np.log(y_hat.T)+(1-y)*np.log(1-y_hat.T))/len(y) \n",
        "    return loss\n",
        "\n",
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.n=28\n",
        "    self.in_ch=1\n",
        "    self.m=10\n",
        "    self.f1=7\n",
        "    self.f2=5\n",
        "    self.cs=1\n",
        "    self.o_ch=5\n",
        "\n",
        "    o2ch=5\n",
        "\n",
        "    self.mxs=2\n",
        "\n",
        "    self.conv1=ConvLayer(in_ch=self.in_ch,out_ch=self.o_ch,kernel=(self.f1,self.f1),stride=self.cs)\n",
        "\n",
        "    self.conv2=ConvLayer(in_ch=self.o_ch,out_ch=o2ch,kernel=(self.f2,self.f2),stride=self.cs)\n",
        "\n",
        "    self.mxp1=Max_Pool_Layer(self.mxs)\n",
        "\n",
        "    cos=(self.n-self.f1+1)//self.mxs-self.f2+1\n",
        "\n",
        "\n",
        "    self.FC1=Neural_Network([o2ch*cos*cos,4,1],activation='sig')\n",
        "\n",
        "  def gen_images(self):\n",
        "    return np.random.randn(self.m,self.in_ch,self.n,self.n)\n",
        "\n",
        "  def update_weights(self,lr,b1=0.9,b2=0.9):\n",
        "    self.conv1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.conv2.update_weights(lr,b1=0.9,b2=0.9)\n",
        "    self.FC1.update_weights(lr,b1=0.9,b2=0.9)\n",
        "\n",
        "  def feedforward(self,x):\n",
        "    c1=self.conv1.feedforward(x)\n",
        "    m1=self.mxp1.feedforward(c1)\n",
        "    c2=self.conv2.feedforward(m1)\n",
        "    f1=self.FC1.feedforward(c2.reshape(c2.shape[0],-1))\n",
        "    return f1\n",
        "  def grad(self,x,y,yhat):\n",
        "    loss_grad=self.FC1.loss_grad(y,yhat)\n",
        "    #loss_grad=2*(yhat-y)\n",
        "    gradF1=self.FC1.grad(self.conv2.out.reshape(loss_grad.shape[0],-1),loss_grad).reshape(self.conv2.out.shape)\n",
        "    #print(gradF1.shape)\n",
        "    #nn.compute_grad(x,loss_grad).sum(axis=0)/2\n",
        "    gradC2=self.conv2.grad(self.mxp1.m,gradF1)\n",
        "    #print(gradC2.shape)\n",
        "    grad_mxp1=self.mxp1.grad(self.conv1.out,gradC2)\n",
        "    gradC1=self.conv1.grad(x,grad_mxp1)\n",
        "    #print(gradC1)\n",
        "\n",
        "  def num_grad(self,x,y):\n",
        "    ep=np.zeros(x.shape)\n",
        "    dw=np.zeros(x.shape)\n",
        "    for dd in range(x.shape[0]):\n",
        "      for k in range(x.shape[1]):\n",
        "       for i in range(x.shape[2]):\n",
        "         for j in range(x.shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           yhat1=self.feedforward(x+ep)\n",
        "           yhat2=self.feedforward(x-ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw\n",
        "\n",
        "  def num_grad_df(self,x,y,shape,func):\n",
        "    #f=self.conv2.filters\n",
        "    #func=self.conv2.perturb_f\n",
        "    ep=np.zeros(shape)\n",
        "    dw=np.zeros(shape)\n",
        "    #print(self.conv2.filters)\n",
        "    for dd in range(shape[0]):\n",
        "      for k in range(shape[1]):\n",
        "       for i in range(shape[2]):\n",
        "         for j in range(shape[3]):\n",
        "\n",
        "           ep[dd,k,i,j]=1e-4\n",
        "           func(ep)\n",
        "           #print(self.conv2.filters)\n",
        "           yhat1=self.feedforward(x)\n",
        "           func(-2*ep)\n",
        "           yhat2=self.feedforward(x)\n",
        "           #print(self.conv2.filters)\n",
        "           func(ep)\n",
        "           #print(yhat1.shape)\n",
        "           dw[dd,k,i,j]=(self.FC1.loss(y,yhat1)-self.FC1.loss(y,yhat2))/2e-4\n",
        "           ep[dd,k,i,j]=0\n",
        "\n",
        "    return dw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwM969W5Q3y",
        "colab_type": "code",
        "outputId": "c009fbcb-ae3c-4928-e879-4a5cf104b844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "np.random.seed(100000)\n",
        "nn=Network()\n",
        "img=nn.gen_images()\n",
        "y=(np.random.randn(img.shape[0])>0)/1\n",
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "loss_grad=nn.FC1.loss_grad(y,yhat)\n",
        "#loss_grad_x=np.einsum('ij,kj->ik',loss_grad,self.w[:-1])\n",
        "x1=nn.FC1.layers[0].output\n",
        "\n",
        "dw=np.einsum('ij,ik->kj',loss_grad,np.c_[x1, np.ones(len(x1))])/len(x1)\n",
        "print(dw)\n",
        "print(x1.shape)\n",
        "print(loss_grad.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 1)\n",
            "[[0.23712643]\n",
            " [0.26059866]\n",
            " [0.16315606]\n",
            " [0.02279548]\n",
            " [0.27899397]]\n",
            "(10, 4)\n",
            "(10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMpxiHW5HH2d",
        "colab_type": "code",
        "outputId": "0ec34c0e-c41d-45c8-c6cb-36423de14f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "nn.FC1.grad_check_weights(x1,y,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03952345]\n",
            " [0.03625335]\n",
            " [0.04442898]\n",
            " [0.05198391]\n",
            " [0.09027377]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.061569217724248365"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sncg_MUFKgg",
        "colab_type": "code",
        "outputId": "6e9250ed-6914-4f06-8850-79f8a775057c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "nn.FC1.grad_check_weights(nn.conv2.out.reshape(nn.conv2.out.shape[0],-1),y,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.05895717]\n",
            " [0.06479381]\n",
            " [0.04055392]\n",
            " [0.00566151]\n",
            " [0.06935103]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.495286466298657e-11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUJvFC3MFlJF",
        "colab_type": "code",
        "outputId": "b3fae128-00fa-4be4-d8ef-5daa5618daec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "nn.FC1.layers[0].dw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00812571, -0.00801027, -0.00533791, -0.00229384],\n",
              "       [-0.01198913, -0.01529798, -0.00604126, -0.00332642],\n",
              "       [-0.00787798, -0.01071905, -0.00333557,  0.00130948],\n",
              "       [-0.00159403, -0.00210315,  0.00064619,  0.00557509],\n",
              "       [ 0.00287991,  0.00348557,  0.00158188,  0.00076234]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EayCNu6M5aic",
        "colab_type": "code",
        "outputId": "f3c8bb79-7a3c-4292-ca61-f40ec3142a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "#np.random.seed(10000)\n",
        "np.random.seed(100000)\n",
        "nn=Network()\n",
        "img=nn.gen_images()\n",
        "y=(np.random.randn(img.shape[0])>0)/1\n",
        "yhat=nn.feedforward(img)\n",
        "\n",
        "nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "#print(nn.conv1.df)\n",
        "#print(nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f))\n",
        "#print(nn.conv1.df/img.shape[0])\n",
        "#print(nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f))\n",
        "epochs=3\n",
        "l=nn.FC1.loss(y,yhat)\n",
        "for epoch in range(epochs):\n",
        "  yhat=nn.feedforward(img)\n",
        "  nn.grad(img,y,yhat)\n",
        "  nn.update_weights(1e-1)\n",
        "  if(epoch%(epochs/10)==1):\n",
        "    print(nn.FC1.loss(y,yhat))\n",
        "print(nn.conv1.df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(99, 4)\n",
            "(5, 1)\n",
            "[[[[-5.11355630e-11  3.68826920e-10  2.07596240e-11  2.54010560e-10\n",
            "     6.31989463e-12  2.57463964e-10 -2.02354794e-10]\n",
            "   [ 3.55838993e-10 -3.83039238e-10 -4.59458484e-10 -5.91663053e-11\n",
            "     5.85848277e-10 -6.00659274e-11 -2.79101750e-10]\n",
            "   [ 9.63982070e-11 -1.28369084e-11  3.10407574e-11  4.03344596e-10\n",
            "     4.06055820e-10 -4.13123860e-12  4.60221868e-11]\n",
            "   [ 2.75674168e-11 -1.66148494e-10 -1.82285987e-10 -4.13095390e-10\n",
            "     1.25644216e-10  8.90532355e-11 -1.84056126e-10]\n",
            "   [-3.28156949e-10  1.99903211e-10  1.89254545e-10  7.23617192e-11\n",
            "    -1.26367473e-10  8.91762270e-11  2.18930829e-10]\n",
            "   [ 5.08697266e-11  2.38622896e-10 -2.09405526e-10  1.49479253e-10\n",
            "     2.05991482e-10 -1.91687854e-10  5.15410058e-12]\n",
            "   [-7.87068694e-11  1.89594009e-10 -1.44494681e-11 -1.94746823e-11\n",
            "     1.36676242e-10  4.10192910e-10 -1.07466814e-10]]]\n",
            "\n",
            "\n",
            " [[[ 1.34317840e-10  9.29748964e-11  1.85180426e-10 -1.25747909e-10\n",
            "    -4.95475310e-10 -1.42321241e-10 -5.90439099e-10]\n",
            "   [-2.52664053e-10  1.82239120e-10  2.10139030e-10  3.49069878e-10\n",
            "    -3.39973423e-10  2.49555573e-10  3.49182015e-10]\n",
            "   [ 1.19260836e-10 -1.60020365e-10 -2.83716974e-11 -3.82324160e-10\n",
            "    -4.68685690e-12 -8.17573252e-13  2.69137044e-11]\n",
            "   [ 2.18740979e-10 -1.01502677e-10 -4.57887783e-10  3.94733761e-10\n",
            "    -3.24540930e-10 -3.45184291e-11 -2.01775637e-11]\n",
            "   [-1.04935220e-10 -4.17165543e-10  2.36943539e-10 -8.26703621e-10\n",
            "    -1.42006017e-10 -5.56422972e-10 -3.42308095e-10]\n",
            "   [-6.59871037e-10  1.74498779e-10  1.47954759e-10 -2.32894018e-10\n",
            "     2.11478029e-11  1.34364611e-10  8.02208944e-11]\n",
            "   [ 4.61306092e-10  3.30604161e-10 -4.13025062e-10  1.38321878e-10\n",
            "     6.88761166e-11 -2.51116823e-10  5.14517787e-11]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQh0SLtsCWBE",
        "colab_type": "code",
        "outputId": "ca70b2b5-e866-4493-aa06-bf38049ce893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "yhat=nn.feedforward(img)\n",
        "nn.grad(img,y,yhat)\n",
        "print(nn.conv2.df)\n",
        "print(nn.num_grad_df(img,y,nn.conv2.filters.shape,nn.conv2.perturb_f))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[-0.00846531 -0.00985575]\n",
            "   [-0.01211816  0.00315272]]]]\n",
            "[[[[-0.00846531 -0.00985575]\n",
            "   [-0.01211816  0.00315272]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0tM-YGi4sng",
        "colab_type": "code",
        "outputId": "d62325a4-16ff-402b-9d1a-c289a79d116c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nn.FC1.layers[0].w.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptUdSGPG5iZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XX=train_x[:1000]\n",
        "XX=np.vstack([XX,train_x[10000:11000]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYP9kzBG5m4D",
        "colab_type": "code",
        "outputId": "0f4cc4cc-6c98-46bd-e50a-49a0105db9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(XX[999])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4e2e038fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOE0lEQVR4nO3df6zddX3H8der9dILBbSVUWuF8SMl\nC0Gt27UySxSCEMAlhbAxusXVDHdNBlMSsshcMtkWN4IDY5wj1kGoE9AmSiCmm9SrCeqQcQuVFsqg\nkhLalRZSDIVBf773x/3iLnDP59ye8z3ney7v5yO5Oed8399zvu+c9NXv93w/53s+jggBeOub1XQD\nAPqDsANJEHYgCcIOJEHYgSTe1s+NHeE5May5/dwkkMqreln7Yq+nqnUVdtsXSPqKpNmS/jUiri+t\nP6y5+pDP7WaTAAoeiLGWtY4P423PlvQ1SRdKOl3SCtund/p6AHqrm8/sSyVtiYinImKfpG9LWl5P\nWwDq1k3YF0l6ZtLjbdWy17E9anvc9vh+7e1icwC60fOz8RGxKiJGImJkSHN6vTkALXQT9u2STpj0\n+D3VMgADqJuwPyhpse2TbR8h6XJJ99TTFoC6dTz0FhEHbF8l6QeaGHq7NSIera0zALXqapw9ItZK\nWltTLwB6iK/LAkkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEH\nkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEX6dsxlvPr/7kd4v1Q7P71MhhOv7ep4v1A9v/p0+d\n9A97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnF2FG37qw8X6w9ceVOxPsdDdbZTm2W/f3mxPu/j\nfWqkj7oKu+2tkvZIOijpQESM1NEUgPrVsWc/JyKer+F1APQQn9mBJLoNe0i61/Z626NTrWB71Pa4\n7fH92tvl5gB0qtvD+LMiYrvt4yWts/14RNw3eYWIWCVplSQd6/nR5fYAdKirPXtEbK9ud0m6S9LS\nOpoCUL+Ow257ru1jXrsv6XxJm+pqDEC9ujmMXyDpLtuvvc4dEfEftXSF15l11FHF+svnndGydvAI\nF59785e+Uqy/e/bPivU5Hi7WB9VPltxRrH907Ypi/Z1/9r/F+iBeD99x2CPiKUnvr7EXAD3E0BuQ\nBGEHkiDsQBKEHUiCsANJcInrDPDk372vWN+84mtdvHq7S1AH8xLVbs1qs5/7yfu/U6wv+9hVxfq8\n1YM39MaeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJy9D2bNnVusP/HF9xbr9196Y5stzMzLTGey\nG/7m68X6P64ufzeiCezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtn7oN04+uN/0O56dMbRB82y\n4f1Nt3DY2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs09TadpkL3pX8bk3XFSeHngmG3ulPJ30\nVXf9acvaty795+Jzf2dORy2hhbZ7dtu32t5le9OkZfNtr7P9ZHU7r7dtAujWdA7jb5N0wRuWXStp\nLCIWSxqrHgMYYG3DHhH3Sdr9hsXLJa2u7q+WdHHNfQGoWaef2RdExI7q/rOSFrRa0faopFFJGlb5\n8x2A3un6bHxEhKQo1FdFxEhEjAyJMy5AUzoN+07bCyWput1VX0sAeqHTsN8jaWV1f6Wku+tpB0Cv\ntP3MbvtOSWdLOs72NklfkHS9pDW2r5D0tKTLetnkIHj5vDNa1sb+5eY+dtJfl275eLG+7/wXivVT\n997fsvbiJe2u03+1Tb05v/f48jZrbOtLH4ejbdgjYkWL0rk19wKgh/i6LJAEYQeSIOxAEoQdSIKw\nA0lwiWtl1nB5GOjEzz3Rp07q9cKh8vDVJddcU6y/48EdxXrs3Vusl97X2T5UfO4ge+GbJxTr8wZw\n6I09O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kkWacffZppxbrx61+rli/5cSxOtvpm089dWmxfvSa\nnxfrB9q8/ttOOalYn3Pbyy1rHxne1+bVm3Pnnpa/tCZJOuaZwe29FfbsQBKEHUiCsANJEHYgCcIO\nJEHYgSQIO5BEmnH255YdX6zffeKaPnVSv9LPPR/8ZG9n4Tly9UvF+p2n/KCn2++VL/7iomL9pB+t\n71Mn9WHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJpBlnn8nGXjmqWC9Om3zwYPG5ey4/s1h/95Vb\nivWZOo5+zmf+vFg/+fsbivWos5k+abtnt32r7V22N01adp3t7bY3VH/lbyAAaNx0DuNvk3TBFMu/\nHBFLqr+19bYFoG5twx4R90na3YdeAPRQNyforrL9SHWYP6/VSrZHbY/bHt+v8rxgAHqn07DfLOlU\nSUsk7ZB0Y6sVI2JVRIxExMiQentRBoDWOgp7ROyMiIMRcUjSNyQtrbctAHXrKOy2F056eImkTa3W\nBTAY2o6z275T0tmSjrO9TdIXJJ1te4kmhhu3Svp0D3usxYKVW5tuoWMH5WK9NEf603/74eJzN37q\nqx31NBP84S+nGkSa8PaHdxafe6DNvPMzUduwR8SKKRbf0oNeAPQQX5cFkiDsQBKEHUiCsANJEHYg\niTSXuH7/tH8v1vcP8DWLHx3+VbE+tOXRlrUlc37W5tWHO+hoMNy0+7eK9c0/XNyyduJT/1l3OwOP\nPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJJFmnP19/zXVxXv/b/0Hv9WnTg7fHA8V6+cc+WqhOnPH\n0dsZGy1fvnvi/fnG0kvYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEmnG2efc/Y7yCh/sTx+Yvof3\nHSrWZ716oFgf4J8oaAR7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IIs04O2aeP17zmWL9lIfv71Mn\nbw1t9+y2T7D9Y9uP2X7U9mer5fNtr7P9ZHU7r/ftAujUdA7jD0i6JiJOl3SmpCttny7pWkljEbFY\n0lj1GMCAahv2iNgREQ9V9/dI2ixpkaTlklZXq62WdHGvmgTQvcP6zG77JEkfkPSApAURsaMqPStp\nQYvnjEoalaRhHdVpnwC6NO2z8baPlvRdSVdHxIuTaxERanHdQUSsioiRiBgZ0pyumgXQuWmF3faQ\nJoJ+e0R8r1q80/bCqr5Q0q7etAigDm0P421b0i2SNkfETZNK90haKen66vbunnRYk9n7yhc8vnCo\n9HPM0rxZg/uTzM8ffKVlbdjl/8+PntXbo61tB1r3tmD2ET3dNl5vOp/Zl0n6hKSNtjdUyz6viZCv\nsX2FpKclXdabFgHUoW3YI+KnktyifG697QDoFb4uCyRB2IEkCDuQBGEHkiDsQBJpLnF9++0/L9Y/\n9q6/LNZ/dPWXWr92l2Pw//D8e4v1M47cVqz//Vf/omXtlePb/KDyaS+X6106/jtHtqxtu7D8U9EL\nNvJj0HVizw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXjiR2b641jPjw95Zl4o9+IfndmydnCo1UWB\n0zP/sZeK9X3zytecD9073tX28dbxQIzpxdg95T9I9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESa\n69m7dewd5evhu9Humw5DPdsyMmHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJtA277RNs/9j2Y7Yf\ntf3Zavl1trfb3lD9XdT7dgF0ajpfqjkg6ZqIeMj2MZLW215X1b4cEf/Uu/YA1GU687PvkLSjur/H\n9mZJi3rdGIB6HdZndtsnSfqApAeqRVfZfsT2rbbntXjOqO1x2+P7tberZgF0btpht320pO9Kujoi\nXpR0s6RTJS3RxJ7/xqmeFxGrImIkIkaGVP4tNQC9M62w2x7SRNBvj4jvSVJE7IyIgxFxSNI3JC3t\nXZsAujWds/GWdIukzRFx06TlCyetdomkTfW3B6Au0zkbv0zSJyRttL2hWvZ5SStsL9HEFZpbJX26\nJx0CqMV0zsb/VNJUv0O9tv52APQK36ADkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxA\nEoQdSIKwA0kQdiAJwg4k4Yh2EwbXuDH7OUlPT1p0nKTn+9bA4RnU3ga1L4neOlVnb78ZEb8xVaGv\nYX/Txu3xiBhprIGCQe1tUPuS6K1T/eqNw3ggCcIOJNF02Fc1vP2SQe1tUPuS6K1Tfemt0c/sAPqn\n6T07gD4h7EASjYTd9gW2/9v2FtvXNtFDK7a32t5YTUM93nAvt9reZXvTpGXzba+z/WR1O+Ucew31\nNhDTeBemGW/0vWt6+vO+f2a3PVvSE5LOk7RN0oOSVkTEY31tpAXbWyWNRETjX8Cw/RFJL0n6ZkSc\nUS27QdLuiLi++o9yXkR8bkB6u07SS01P413NVrRw8jTjki6W9Ek1+N4V+rpMfXjfmtizL5W0JSKe\nioh9kr4taXkDfQy8iLhP0u43LF4uaXV1f7Um/rH0XYveBkJE7IiIh6r7eyS9Ns14o+9doa++aCLs\niyQ9M+nxNg3WfO8h6V7b622PNt3MFBZExI7q/rOSFjTZzBTaTuPdT2+YZnxg3rtOpj/vFifo3uys\niPhtSRdKurI6XB1IMfEZbJDGTqc1jXe/TDHN+K81+d51Ov15t5oI+3ZJJ0x6/J5q2UCIiO3V7S5J\nd2nwpqLe+doMutXtrob7+bVBmsZ7qmnGNQDvXZPTnzcR9gclLbZ9su0jJF0u6Z4G+ngT23OrEyey\nPVfS+Rq8qajvkbSyur9S0t0N9vI6gzKNd6tpxtXwe9f49OcR0fc/SRdp4oz8LyX9dRM9tOjrFEm/\nqP4ebbo3SXdq4rBuvybObVwh6Z2SxiQ9KemHkuYPUG//JmmjpEc0EayFDfV2liYO0R+RtKH6u6jp\n967QV1/eN74uCyTBCTogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/ABx2DPjexGbwAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYGdR4Kw86as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "YY=train_y[:1000]\n",
        "YY=np.vstack([YY,train_y[10000:11000]]).flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBPCde2C99WM",
        "colab_type": "code",
        "outputId": "69481eea-eb8b-4aea-db14-0c9467d419e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(XX[1000])\n",
        "print(YY[1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JcJIrwl9VyW",
        "colab_type": "code",
        "outputId": "df7d3d93-df40-42da-e36b-728471fc1775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "YY.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edjt6YT4sCwX",
        "colab_type": "code",
        "outputId": "900cf3a3-90cb-462c-dfe3-038c4601ea74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "nn=Network()\n",
        "img=XX[:,np.newaxis,:,:]\n",
        "y=YY\n",
        "yhat=nn.feedforward(img)\n",
        "print(yhat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-b6550ca0d894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-39a658385e4f>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmxp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-39a658385e4f>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     self.m=x.flatten()[self.m_indices].reshape(-1,x.shape[1],\n\u001b[1;32m    130\u001b[0m                                                int(x.shape[2]/self.stride),int(x.shape[3]/self.stride))\n",
            "\u001b[0;32m<ipython-input-2-8dab3a956e98>\u001b[0m in \u001b[0;36mmx_pool\u001b[0;34m(img, s)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mind1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#probably i need to change s in the middle to change vertical stride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 729000 into shape (2,13)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiozU9zd1RQc",
        "colab_type": "code",
        "outputId": "d8fa533c-9b2b-41e1-81eb-a3adfa193f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "yhat=nn.feedforward(img)\n",
        "print(yhat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 2000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUNeRhY0p8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.grad(img,y,yhat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlN_-jok0zHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.update_weights(1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ys2k1hK1Avz",
        "colab_type": "code",
        "outputId": "552581b9-c74b-44ae-be2b-19f8558408a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nn.conv2.filters.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5, 5, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOzWNEiv9B1v",
        "colab_type": "code",
        "outputId": "568c47f9-eab8-46cc-ad22-3579a9258147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "#np.random.seed(10000)\n",
        "nn=Network()\n",
        "#img=nn.gen_images()\n",
        "img=XX[:,np.newaxis,:,:]\n",
        "#y=np.random.randint(2,size=img.shape[0]).reshape(img.shape[0],1)\n",
        "y=YY\n",
        "yhat=nn.feedforward(img)\n",
        "print(yhat.shape)\n",
        "#nn.grad(img,y,yhat)\n",
        "#nn.num_grad(img,y)\n",
        "#print(nn.conv1.df/img.shape[0])\n",
        "#print(nn.num_grad_df(img,y,nn.conv1.filters.shape,nn.conv1.perturb_f))\n",
        "epochs=1000\n",
        "l=nn.FC1.loss(y,yhat)\n",
        "for epoch in range(epochs):\n",
        "  yhat=nn.feedforward(img)\n",
        "  nn.grad(img,y,yhat)\n",
        "  nn.update_weights(1e-1)\n",
        "  if(epoch%(epochs/10)==1):\n",
        "    print(nn.FC1.loss(y,yhat))\n",
        "#print(nn.conv1.df/img.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(246, 4)\n",
            "(5, 1)\n",
            "(2000, 1)\n",
            "0.6523515777322453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-31201c30d8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-161-2002ab4c3809>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, x, y, yhat)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m#print(gradF1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m#nn.compute_grad(x,loss_grad).sum(axis=0)/2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mgradC2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmxp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradF1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;31m#print(gradC2.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mgrad_mxp1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmxp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradC2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-161-2002ab4c3809>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, x, loss_grad)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrided_convolution3D_grad1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mgg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8dab3a956e98>\u001b[0m in \u001b[0;36mstrided_convolution3D_grad1\u001b[0;34m(image, weight, stride)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#print(windows[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#print(windows*gg1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mcopjk,mejk->mecop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[0;31m# If no optimization, run pure einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimize_arg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0mvalid_einsum_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'order'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'casting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miq0BrFxryOi",
        "colab_type": "code",
        "outputId": "3b543f7d-e9c6-405d-826f-a8418df93c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "yhat=nn.feedforward(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8sSN6rEt_Yn",
        "colab_type": "code",
        "outputId": "07ed8f85-8538-43af-9a91-96b90de3192e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMCYGUH6z6Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=np.argmax(yhat>0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBRuySAjNCu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=yhat>0.5\n",
        "ddd=ind-y.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAb1mKfiNHYB",
        "colab_type": "code",
        "outputId": "84c926c1-229e-4090-808e-de970b68eddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(ddd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbnBkn1pNayn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}